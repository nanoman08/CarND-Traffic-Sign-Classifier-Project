{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor\n",
    "In TensorFlow, data isn’t stored as integers, floats, or strings. These values are encapsulated in an object called a tensor. In the case of hello_constant = tf.constant('Hello World!'), hello_constant is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A is a 0-dimensional int32 tensor\n",
    "A = tf.constant(1234) \n",
    "# B is a 1-dimensional int32 tensor\n",
    "B = tf.constant([123,456,789]) \n",
    " # C is a 2-dimensional int32 tensor\n",
    "C = tf.constant([ [123,456,789], [222,333,444] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Input  \n",
    "In the last section, you passed a tensor into a session and it returned the result. What if you want to use a non-constant? This is where tf.placeholder() and feed_dict come into place. In this section, let's go over the basics of feeding data into TensorFlow.\n",
    "\n",
    "tf.placeholder()  \n",
    "Sadly you can’t just set x to your dataset and put it in TensorFlow, because over time you'll want your TensorFlow model to take in different datasets with different parameters. You need tf.placeholder()!\n",
    "\n",
    "tf.placeholder() returns a tensor that gets its value from data passed to the tf.session.run() function, allowing you to set the input right before the session runs.\n",
    "\n",
    "Session’s feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "123.12000274658203\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})\n",
    "    print(output)\n",
    "    \n",
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z, feed_dict={x: 'Test String', y: 123, z: '123.12'})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting types  \n",
    "It may be necessary to convert between types to make certain operators work together. For example, if you tried the following, it would fail with an exception:  \n",
    "\n",
    "tf.subtract(tf.constant(2.0),tf.constant(1))  \n",
    " results in the error message: <font color='red'>Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32:</font>\n",
    "That's because the constant 1 is an integer but the constant 2.0 is a floating point value and subtract expects them to match.\n",
    "\n",
    "In cases like these, you can either make sure your data is all of the same type, or you can cast a value to another type. In this case, converting the 2.0 to an integer before subtracting, like so, will give the correct result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.add(5, 2) \n",
    "type(x)\n",
    "\n",
    "x= tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x)\n",
    "    print(output)\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x,y), 1)\n",
    "\n",
    "# TODO: Print z from a session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposition\n",
    "We've been using the y = Wx + b function for our linear function.\n",
    "\n",
    "But there's another function that does the same thing, y = xW + b. These functions do the same thing and are interchangeable, except for the dimensions of the matrices involved.\n",
    "\n",
    "To shift from one function to the other, you simply have to swap the row and column dimensions of each matrix. This is called transposition.\n",
    "\n",
    "For rest of this lesson, we actually use xW + b, because this is what TensorFlow uses.\n",
    "y = xW + b\n",
    "y = xW + b\n",
    "The above example is identical to the quiz you just completed, except that the matrices are transposed.\n",
    "\n",
    "x now has the dimensions 1x3, W now has the dimensions 3x2, and b now has the dimensions 1x2. Calculating this will produce a matrix with the dimension of 1x2.\n",
    "\n",
    "You'll notice that the elements in this 1x2 matrix are the same as the elements in the 2x1 matrix from the quiz. Again, these matrices are simply transposed.\n",
    "\n",
    "We now have our logits! The columns represent the logits for our two labels.\n",
    "\n",
    "Now you can learn how to train this function in TensorFlow.\n",
    "\n",
    "Weights and Bias in TensorFlow\n",
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out tf.placeholder() and tf.constant(), since those Tensors can't be modified. This is where tf.Variable class comes in.\n",
    "tf.Variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65900117  0.24243298  0.09856589]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "output = None\n",
    "logit_data = [2.0, 1.0, 0.1]\n",
    "logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "# TODO: Calculate the softmax of the logits\n",
    "# softmax = \n",
    "softmax = tf.nn.softmax(logits)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "        \n",
    "    # TODO: Feed in the logit data\n",
    "    # output = sess.run(softmax,    )\n",
    "    output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.11000013   8.44000053]\n",
      " [  0.           0.        ]\n",
      " [ 24.01000214  38.23999786]]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features,weights[0]),biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "output_node = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "# TODO: Print session results\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    output = sess.run(output_node)\n",
    "    \n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network in TensorFlow\n",
    "You've seen how to build a logistic classifier using TensorFlow. Now you're going to see how to use the logistic classifier to build a deep neural network.\n",
    "\n",
    "## Step by Step\n",
    "In the following walkthrough, we'll step through TensorFlow code written to classify the letters in the MNIST database. If you would like to run the network on your computer, the file is provided here. You can find this and many more examples of TensorFlow at Aymeric Damien's GitHub repository: https://github.com/aymericdamien/TensorFlow-Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "n_hidden_layer = 256 # layer number of features\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "\n",
    "# Hidden layer with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\\\n",
    "    biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "# Output layer with linear activation\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "    \n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph/session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Restore TensorFlow Models\n",
    "Training a model can take hours. But once you close your TensorFlow session, you lose all the trained weights and biases. If you were to reuse the model in the future, you would have to train it all over again!\n",
    "\n",
    "Fortunately, TensorFlow gives you the ability to save your progress using a class called tf.train.Saver. This class provides the functionality to save any tf.Variable to your file system.\n",
    "\n",
    "Saving Variables\n",
    "Let's start with a simple example of saving weights and bias Tensors. For the first example you'll just save two variables. Later examples will save all the weights in a practical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[[ 0.76766264 -0.88123387 -0.87017906]\n",
      " [ 0.30894622  0.23104727  1.05577505]]\n",
      "Bias:\n",
      "[ 1.07439768  1.38222504 -0.07303043]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The file path to save the data\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all the Variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    print('Weights:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(bias))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:\n",
      "[[-1.71175706  0.9501105   0.00879494]\n",
      " [ 0.17054018 -1.2989341   1.72880495]]\n",
      "Bias:\n",
      "[ 0.1154167  -1.53356671  0.23361027]\n"
     ]
    }
   ],
   "source": [
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "save_file = './model.ckpt'\n",
    "# Two Variables: weights and bias, I don't understand why we need to create the variable here\n",
    "# should them not be saved and be stored from the model.ckpt file?\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    print('Weight:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n",
      "Epoch 0   - Validation Accuracy: 0.1120000034570694\n",
      "Epoch 10  - Validation Accuracy: 0.24660000205039978\n",
      "Epoch 20  - Validation Accuracy: 0.4018000066280365\n",
      "Epoch 30  - Validation Accuracy: 0.5116000175476074\n",
      "Epoch 40  - Validation Accuracy: 0.5752000212669373\n",
      "Epoch 50  - Validation Accuracy: 0.621999979019165\n",
      "Epoch 60  - Validation Accuracy: 0.6539999842643738\n",
      "Epoch 70  - Validation Accuracy: 0.6773999929428101\n",
      "Epoch 80  - Validation Accuracy: 0.6985999941825867\n",
      "Epoch 90  - Validation Accuracy: 0.7161999940872192\n",
      "Trained Model Saved.\n"
     ]
    }
   ],
   "source": [
    "#Save a Trained Model\n",
    "#Let's see how to train a model and save its weights.\n",
    "#First start with a model:\n",
    "# Remove previous Tensors and Operations\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('.', one_hot=True)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "import math\n",
    "\n",
    "save_file = './train_model.ckpt'\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        total_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_features, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            sess.run(\n",
    "                optimizer,\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Print status for every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            valid_accuracy = sess.run(\n",
    "                accuracy,\n",
    "                feed_dict={\n",
    "                    features: mnist.validation.images,\n",
    "                    labels: mnist.validation.labels})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {}'.format(\n",
    "                epoch,\n",
    "                valid_accuracy))\n",
    "    \n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7333999872207642\n"
     ]
    }
   ],
   "source": [
    "#Load a Trained Model\n",
    "#Let's load the weights and bias from memory, then check the test accuracy.\n",
    "saver = tf.train.Saver()\n",
    "save_file = './train_model.ckpt'\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Weights and Biases into a New Model\n",
    "Sometimes you might want to adjust, or \"finetune\" a model that you have already trained and saved.\n",
    "\n",
    "However, loading saved Variables directly into a modified model can generate errors. Let's go over how to avoid these problems.\n",
    "\n",
    "### Naming Error\n",
    "TensorFlow uses a string identifier for Tensors and Operations called name. If a name is not given, TensorFlow will create one automatically. TensorFlow will give the first node the name <Type>, and then give the name <Type>_<number> for the subsequent nodes. Let's see how this can affect loading a model with a different order of weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Weights: Variable:0\n",
      "Save Bias: Variable_1:0\n",
      "Load Weights: Variable_1:0\n",
      "Load Bias: Variable:0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable, save/RestoreV2)]]\n\nCaused by op 'save/Assign', defined at:\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-60738c19c262>\", line 29, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1000, in __init__\n    self.build()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 373, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 130, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable, save/RestoreV2)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable, save/RestoreV2)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-60738c19c262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[1;31m# Load the weights and bias - ERROR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1386\u001b[0m       \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1388\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable, save/RestoreV2)]]\n\nCaused by op 'save/Assign', defined at:\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-60738c19c262>\", line 29, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1000, in __init__\n    self.build()\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 373, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 130, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Chou_h\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable, save/RestoreV2)]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = '.\\model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - ERROR\n",
    "    saver.restore(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the name properties for weights and bias are different than when you saved the model. This is why the code produces the \"Assign requires shapes of both tensors to match\" error. The code saver.restore(sess, save_file) is trying to load weight data into bias and bias data into weights.\n",
    "\n",
    "Instead of letting TensorFlow set the name property, let's set it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Weights: weights_0:0\n",
      "Save Bias: bias_0:0\n",
      "Load Weights: weights_0:0\n",
      "Load Bias: bias_0:0\n",
      "Weights: [[ 1.19711983  0.60525662  0.01975811]\n",
      " [-1.13226259  0.40896446  0.57845569]]\n",
      "bias: [-0.3491174   0.4997735  -1.89347661]\n",
      "Loaded Weights and Bias successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = '.\\model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]), name='weights_0')\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]) ,name='weights_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - No Error\n",
    "    saver.restore(sess, save_file)\n",
    "    print('Weights: {}'.format(sess.run(weights)))\n",
    "    print('bias: {}'.format(sess.run(bias)))\n",
    "\n",
    "print('Loaded Weights and Bias successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Drop Out\n",
    "#### drop out can be an ensemble of learning from a variety of combination of neurons, in each forwad/backward propogation, certian neurons (at given probablity) are dropped from the network, but are added back in during test/validation\n",
    "This quiz will be starting with the code from the ReLU Quiz and applying a dropout layer. Build a model with a ReLU layer and dropout layer using the keep_prob placeholder to pass in a probability of 0.5. Print the logits from the model.\n",
    "\n",
    "Note: Output will be different every time the code is run. This is caused by dropout randomizing the units it drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.31111097  8.88888931]\n",
      " [ 0.56777787  0.93777788]\n",
      " [ 5.31111097  2.65555549]]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model with Dropout\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob )\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print logits from a session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    output = sess.run(logits, feed_dict={keep_prob: 0.9})\n",
    "    print(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TensorFlow Convolution Layer\n",
    "Let's examine how to implement a CNN in TensorFlow.\n",
    "\n",
    "TensorFlow provides the tf.nn.conv2d() and tf.nn.bias_add() functions to create your own convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Output depth\n",
    "k_output = 64\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Input/Image\n",
    "input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "# Weight and bias\n",
    "weight = tf.Variable(tf.truncated_normal(\n",
    "    [filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "# Apply Convolution\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "# Add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "# Apply activation function\n",
    "conv_layer = tf.nn.relu(conv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code above uses the tf.nn.conv2d() function to compute the convolution with weight as the filter and [1, 2, 2, 1] for the strides. TensorFlow uses a stride for each input dimension, [batch, input_height, input_width, input_channels]. We are generally always going to set the stride for batch and input_channels (i.e. the first and fourth element in the strides array) to be 1.\n",
    "\n",
    "You'll focus on changing input_height and input_width while setting batch and input_channels to 1. The input_height and input_width strides are for striding the filter over input. This example code uses a stride of 2 with 5x5 filter over input.\n",
    "\n",
    "The tf.nn.bias_add() function adds a 1-d bias to the last dimension in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1.09643614  2.17616415  0.60052419  1.36383855  1.74957633]\n",
      "   [ 1.5408262   1.53479958  0.77369386  0.93947649  1.23991013]]\n",
      "\n",
      "  [[ 2.18688726  0.64697164  0.5648331   2.31544542  1.17406058]\n",
      "   [ 0.43191442  0.80306947  1.2853024   1.38294697  0.39325324]]]]\n"
     ]
    }
   ],
   "source": [
    "# pooling, pool layer size = 2x2x5\n",
    "import numpy as np\n",
    "\n",
    "input = tf.placeholder(tf.float32, (None, 4, 4, 5))\n",
    "filter_shape = [1, 2, 2, 1]\n",
    "strides = [1, 2, 2, 1]\n",
    "padding = 'VALID'\n",
    "pool = tf.nn.max_pool(input, filter_shape, strides, padding)\n",
    "x =  np.random.normal(size=(1,4,4,5))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    output = sess.run(pool, feed_dict={input:x})\n",
    "    #print(x)\n",
    "    print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Network in TensorFlow\n",
    "It's time to walk through an example Convolutional Neural Network (CNN) in TensorFlow.\n",
    "\n",
    "The structure of this network follows the classic structure of CNNs, which is a mix of convolutional layers and max pooling, followed by fully-connected layers.\n",
    "\n",
    "The code you'll be looking at is similar to what you saw in the segment on Deep Neural Network in TensorFlow, except we restructured the architecture of this network as a CNN.\n",
    "\n",
    "Just like in that segment, here you'll study the line-by-line breakdown of the code. If you want, you can even download the code and run it yourself.\n",
    "\n",
    "Thanks to Aymeric Damien for providing the original TensorFlow model on which this segment is based.\n",
    "\n",
    "Time to dive in!\n",
    "\n",
    "Dataset\n",
    "You've seen this section of code from previous lessons. Here we're importing the MNIST dataset and using a convenient TensorFlow function to batch, scale, and One-Hot encode the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n",
      "Epoch  1, Batch   1 -Loss: 47797.6406 Validation Accuracy: 0.117188\n",
      "Epoch  1, Batch   2 -Loss: 38293.3047 Validation Accuracy: 0.117188\n",
      "Epoch  1, Batch   3 -Loss: 35045.6094 Validation Accuracy: 0.125000\n",
      "Epoch  1, Batch   4 -Loss: 31092.2266 Validation Accuracy: 0.125000\n",
      "Epoch  1, Batch   5 -Loss: 30382.3301 Validation Accuracy: 0.125000\n",
      "Epoch  1, Batch   6 -Loss: 28278.2695 Validation Accuracy: 0.148438\n",
      "Epoch  1, Batch   7 -Loss: 25594.3867 Validation Accuracy: 0.152344\n",
      "Epoch  1, Batch   8 -Loss: 24180.5898 Validation Accuracy: 0.164062\n",
      "Epoch  1, Batch   9 -Loss: 24049.7012 Validation Accuracy: 0.175781\n",
      "Epoch  1, Batch  10 -Loss: 22452.4570 Validation Accuracy: 0.171875\n",
      "Epoch  1, Batch  11 -Loss: 20923.4141 Validation Accuracy: 0.175781\n",
      "Epoch  1, Batch  12 -Loss: 19086.4648 Validation Accuracy: 0.175781\n",
      "Epoch  1, Batch  13 -Loss: 19398.0684 Validation Accuracy: 0.171875\n",
      "Epoch  1, Batch  14 -Loss: 18675.8555 Validation Accuracy: 0.171875\n",
      "Epoch  1, Batch  15 -Loss: 17291.8105 Validation Accuracy: 0.171875\n",
      "Epoch  1, Batch  16 -Loss: 17759.0039 Validation Accuracy: 0.179688\n",
      "Epoch  1, Batch  17 -Loss: 16301.7861 Validation Accuracy: 0.187500\n",
      "Epoch  1, Batch  18 -Loss: 16442.1367 Validation Accuracy: 0.187500\n",
      "Epoch  1, Batch  19 -Loss: 15926.4707 Validation Accuracy: 0.203125\n",
      "Epoch  1, Batch  20 -Loss: 14815.2949 Validation Accuracy: 0.203125\n",
      "Epoch  1, Batch  21 -Loss: 13074.3965 Validation Accuracy: 0.210938\n",
      "Epoch  1, Batch  22 -Loss: 14009.6406 Validation Accuracy: 0.210938\n",
      "Epoch  1, Batch  23 -Loss: 13885.7480 Validation Accuracy: 0.214844\n",
      "Epoch  1, Batch  24 -Loss: 13675.2832 Validation Accuracy: 0.218750\n",
      "Epoch  1, Batch  25 -Loss: 14606.2900 Validation Accuracy: 0.218750\n",
      "Epoch  1, Batch  26 -Loss: 13298.1797 Validation Accuracy: 0.222656\n",
      "Epoch  1, Batch  27 -Loss: 13051.1230 Validation Accuracy: 0.246094\n",
      "Epoch  1, Batch  28 -Loss: 11277.5400 Validation Accuracy: 0.250000\n",
      "Epoch  1, Batch  29 -Loss: 12465.9941 Validation Accuracy: 0.265625\n",
      "Epoch  1, Batch  30 -Loss: 12253.1738 Validation Accuracy: 0.269531\n",
      "Epoch  1, Batch  31 -Loss: 11024.5820 Validation Accuracy: 0.281250\n",
      "Epoch  1, Batch  32 -Loss: 11094.0977 Validation Accuracy: 0.308594\n",
      "Epoch  1, Batch  33 -Loss: 10815.0137 Validation Accuracy: 0.304688\n",
      "Epoch  1, Batch  34 -Loss: 11446.1064 Validation Accuracy: 0.320312\n",
      "Epoch  1, Batch  35 -Loss: 10172.0430 Validation Accuracy: 0.324219\n",
      "Epoch  1, Batch  36 -Loss: 10179.1074 Validation Accuracy: 0.339844\n",
      "Epoch  1, Batch  37 -Loss: 10694.9355 Validation Accuracy: 0.335938\n",
      "Epoch  1, Batch  38 -Loss:  9575.9219 Validation Accuracy: 0.343750\n",
      "Epoch  1, Batch  39 -Loss: 10112.7256 Validation Accuracy: 0.351562\n",
      "Epoch  1, Batch  40 -Loss:  9857.5410 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  41 -Loss: 10202.0020 Validation Accuracy: 0.371094\n",
      "Epoch  1, Batch  42 -Loss:  9956.1133 Validation Accuracy: 0.382812\n",
      "Epoch  1, Batch  43 -Loss:  9624.8418 Validation Accuracy: 0.394531\n",
      "Epoch  1, Batch  44 -Loss: 10376.0420 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  45 -Loss:  8801.4668 Validation Accuracy: 0.406250\n",
      "Epoch  1, Batch  46 -Loss:  8458.4727 Validation Accuracy: 0.402344\n",
      "Epoch  1, Batch  47 -Loss:  8068.5361 Validation Accuracy: 0.406250\n",
      "Epoch  1, Batch  48 -Loss:  8735.9941 Validation Accuracy: 0.406250\n",
      "Epoch  1, Batch  49 -Loss:  8855.9229 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  50 -Loss:  8711.0537 Validation Accuracy: 0.417969\n",
      "Epoch  1, Batch  51 -Loss:  8852.5117 Validation Accuracy: 0.437500\n",
      "Epoch  1, Batch  52 -Loss:  7332.8149 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  53 -Loss:  8944.2324 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  54 -Loss:  7867.0361 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  55 -Loss:  7665.7578 Validation Accuracy: 0.453125\n",
      "Epoch  1, Batch  56 -Loss:  7011.8652 Validation Accuracy: 0.480469\n",
      "Epoch  1, Batch  57 -Loss:  6427.8506 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch  58 -Loss:  7365.9580 Validation Accuracy: 0.480469\n",
      "Epoch  1, Batch  59 -Loss:  7982.7983 Validation Accuracy: 0.480469\n",
      "Epoch  1, Batch  60 -Loss:  7007.7700 Validation Accuracy: 0.492188\n",
      "Epoch  1, Batch  61 -Loss:  7733.3652 Validation Accuracy: 0.496094\n",
      "Epoch  1, Batch  62 -Loss:  6575.6250 Validation Accuracy: 0.503906\n",
      "Epoch  1, Batch  63 -Loss:  6623.9414 Validation Accuracy: 0.503906\n",
      "Epoch  1, Batch  64 -Loss:  7791.7617 Validation Accuracy: 0.503906\n",
      "Epoch  1, Batch  65 -Loss:  5651.2964 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch  66 -Loss:  6965.7363 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch  67 -Loss:  5626.6016 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch  68 -Loss:  7248.3936 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch  69 -Loss:  6240.8633 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch  70 -Loss:  6185.7002 Validation Accuracy: 0.535156\n",
      "Epoch  1, Batch  71 -Loss:  6344.7344 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch  72 -Loss:  6809.2915 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch  73 -Loss:  5292.0703 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch  74 -Loss:  6254.8223 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch  75 -Loss:  6425.0454 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch  76 -Loss:  5940.0913 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch  77 -Loss:  5663.4888 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch  78 -Loss:  5432.3857 Validation Accuracy: 0.546875\n",
      "Epoch  1, Batch  79 -Loss:  5342.9814 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch  80 -Loss:  6565.3237 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch  81 -Loss:  5664.9180 Validation Accuracy: 0.546875\n",
      "Epoch  1, Batch  82 -Loss:  5763.8950 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch  83 -Loss:  5324.1001 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch  84 -Loss:  5787.0200 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch  85 -Loss:  5131.2373 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch  86 -Loss:  5830.2168 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch  87 -Loss:  6077.9141 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch  88 -Loss:  6414.0747 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch  89 -Loss:  5685.0303 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch  90 -Loss:  5386.0337 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch  91 -Loss:  5534.1738 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch  92 -Loss:  4690.2256 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch  93 -Loss:  5240.8452 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch  94 -Loss:  5473.6006 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch  95 -Loss:  4770.0146 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch  96 -Loss:  5055.7261 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch  97 -Loss:  4892.0312 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch  98 -Loss:  4284.3545 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch  99 -Loss:  3844.5481 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 100 -Loss:  4283.5498 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 101 -Loss:  4808.2812 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 102 -Loss:  3617.5662 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 103 -Loss:  4346.3398 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 104 -Loss:  4135.4199 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 105 -Loss:  4060.9375 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 106 -Loss:  3198.0176 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 107 -Loss:  4230.3662 Validation Accuracy: 0.613281\n",
      "Epoch  2, Batch   1 -Loss:  4615.2031 Validation Accuracy: 0.613281\n",
      "Epoch  2, Batch   2 -Loss:  4449.5361 Validation Accuracy: 0.605469\n",
      "Epoch  2, Batch   3 -Loss:  4410.1689 Validation Accuracy: 0.609375\n",
      "Epoch  2, Batch   4 -Loss:  4216.0220 Validation Accuracy: 0.613281\n",
      "Epoch  2, Batch   5 -Loss:  4029.5303 Validation Accuracy: 0.605469\n",
      "Epoch  2, Batch   6 -Loss:  4229.9502 Validation Accuracy: 0.609375\n",
      "Epoch  2, Batch   7 -Loss:  4433.4341 Validation Accuracy: 0.617188\n",
      "Epoch  2, Batch   8 -Loss:  3421.7986 Validation Accuracy: 0.613281\n",
      "Epoch  2, Batch   9 -Loss:  4051.5278 Validation Accuracy: 0.621094\n",
      "Epoch  2, Batch  10 -Loss:  4571.2788 Validation Accuracy: 0.625000\n",
      "Epoch  2, Batch  11 -Loss:  4059.8928 Validation Accuracy: 0.628906\n",
      "Epoch  2, Batch  12 -Loss:  4258.7251 Validation Accuracy: 0.617188\n",
      "Epoch  2, Batch  13 -Loss:  4255.9663 Validation Accuracy: 0.625000\n",
      "Epoch  2, Batch  14 -Loss:  3963.8755 Validation Accuracy: 0.621094\n",
      "Epoch  2, Batch  15 -Loss:  3933.4263 Validation Accuracy: 0.617188\n",
      "Epoch  2, Batch  16 -Loss:  3601.2622 Validation Accuracy: 0.625000\n",
      "Epoch  2, Batch  17 -Loss:  3837.3682 Validation Accuracy: 0.625000\n",
      "Epoch  2, Batch  18 -Loss:  3829.7363 Validation Accuracy: 0.625000\n",
      "Epoch  2, Batch  19 -Loss:  3646.2173 Validation Accuracy: 0.625000\n",
      "Epoch  2, Batch  20 -Loss:  3682.9373 Validation Accuracy: 0.617188\n",
      "Epoch  2, Batch  21 -Loss:  4528.1011 Validation Accuracy: 0.632812\n",
      "Epoch  2, Batch  22 -Loss:  4187.9048 Validation Accuracy: 0.625000\n",
      "Epoch  2, Batch  23 -Loss:  3589.3157 Validation Accuracy: 0.628906\n",
      "Epoch  2, Batch  24 -Loss:  3499.4326 Validation Accuracy: 0.636719\n",
      "Epoch  2, Batch  25 -Loss:  3474.7830 Validation Accuracy: 0.621094\n",
      "Epoch  2, Batch  26 -Loss:  3373.2693 Validation Accuracy: 0.628906\n",
      "Epoch  2, Batch  27 -Loss:  3818.7324 Validation Accuracy: 0.628906\n",
      "Epoch  2, Batch  28 -Loss:  3425.0071 Validation Accuracy: 0.648438\n",
      "Epoch  2, Batch  29 -Loss:  3350.3816 Validation Accuracy: 0.648438\n",
      "Epoch  2, Batch  30 -Loss:  3526.0591 Validation Accuracy: 0.648438\n",
      "Epoch  2, Batch  31 -Loss:  3519.1135 Validation Accuracy: 0.656250\n",
      "Epoch  2, Batch  32 -Loss:  3775.1694 Validation Accuracy: 0.656250\n",
      "Epoch  2, Batch  33 -Loss:  3319.2920 Validation Accuracy: 0.656250\n",
      "Epoch  2, Batch  34 -Loss:  3351.5654 Validation Accuracy: 0.656250\n",
      "Epoch  2, Batch  35 -Loss:  3927.6602 Validation Accuracy: 0.660156\n",
      "Epoch  2, Batch  36 -Loss:  3328.3403 Validation Accuracy: 0.660156\n",
      "Epoch  2, Batch  37 -Loss:  3874.5435 Validation Accuracy: 0.664062\n",
      "Epoch  2, Batch  38 -Loss:  3463.7080 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  39 -Loss:  3244.6113 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  40 -Loss:  3562.1587 Validation Accuracy: 0.664062\n",
      "Epoch  2, Batch  41 -Loss:  3345.1606 Validation Accuracy: 0.656250\n",
      "Epoch  2, Batch  42 -Loss:  3531.2058 Validation Accuracy: 0.660156\n",
      "Epoch  2, Batch  43 -Loss:  3245.6887 Validation Accuracy: 0.664062\n",
      "Epoch  2, Batch  44 -Loss:  3470.4443 Validation Accuracy: 0.656250\n",
      "Epoch  2, Batch  45 -Loss:  3360.5625 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  46 -Loss:  3755.6748 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  47 -Loss:  3363.6934 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  48 -Loss:  2934.1379 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  49 -Loss:  3845.2690 Validation Accuracy: 0.675781\n",
      "Epoch  2, Batch  50 -Loss:  3252.9961 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  51 -Loss:  3300.4204 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  52 -Loss:  3036.2195 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  53 -Loss:  2852.9036 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  54 -Loss:  3387.2983 Validation Accuracy: 0.675781\n",
      "Epoch  2, Batch  55 -Loss:  2920.8491 Validation Accuracy: 0.664062\n",
      "Epoch  2, Batch  56 -Loss:  3071.7158 Validation Accuracy: 0.660156\n",
      "Epoch  2, Batch  57 -Loss:  2643.7480 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  58 -Loss:  3181.2671 Validation Accuracy: 0.664062\n",
      "Epoch  2, Batch  59 -Loss:  2815.2461 Validation Accuracy: 0.664062\n",
      "Epoch  2, Batch  60 -Loss:  2872.9919 Validation Accuracy: 0.664062\n",
      "Epoch  2, Batch  61 -Loss:  2886.1936 Validation Accuracy: 0.664062\n",
      "Epoch  2, Batch  62 -Loss:  2965.0493 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  63 -Loss:  3531.5220 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  64 -Loss:  2822.9211 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  65 -Loss:  3112.1646 Validation Accuracy: 0.679688\n",
      "Epoch  2, Batch  66 -Loss:  3102.2537 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  67 -Loss:  3759.9761 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  68 -Loss:  2823.7532 Validation Accuracy: 0.675781\n",
      "Epoch  2, Batch  69 -Loss:  2966.9038 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  70 -Loss:  2717.0339 Validation Accuracy: 0.675781\n",
      "Epoch  2, Batch  71 -Loss:  3064.4292 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  72 -Loss:  2715.5645 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  73 -Loss:  2445.1665 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  74 -Loss:  2633.4976 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  75 -Loss:  3003.0669 Validation Accuracy: 0.664062\n",
      "Epoch  2, Batch  76 -Loss:  3083.6772 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  77 -Loss:  2794.2102 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  78 -Loss:  2956.5835 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  79 -Loss:  2642.9614 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  80 -Loss:  2567.0190 Validation Accuracy: 0.675781\n",
      "Epoch  2, Batch  81 -Loss:  3151.4004 Validation Accuracy: 0.683594\n",
      "Epoch  2, Batch  82 -Loss:  2978.7632 Validation Accuracy: 0.679688\n",
      "Epoch  2, Batch  83 -Loss:  2935.6379 Validation Accuracy: 0.683594\n",
      "Epoch  2, Batch  84 -Loss:  2403.5234 Validation Accuracy: 0.683594\n",
      "Epoch  2, Batch  85 -Loss:  3280.1592 Validation Accuracy: 0.683594\n",
      "Epoch  2, Batch  86 -Loss:  2558.6033 Validation Accuracy: 0.683594\n",
      "Epoch  2, Batch  87 -Loss:  2514.1179 Validation Accuracy: 0.683594\n",
      "Epoch  2, Batch  88 -Loss:  2967.7876 Validation Accuracy: 0.679688\n",
      "Epoch  2, Batch  89 -Loss:  2248.2148 Validation Accuracy: 0.671875\n",
      "Epoch  2, Batch  90 -Loss:  2731.7734 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  91 -Loss:  2628.0862 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  92 -Loss:  2966.4666 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  93 -Loss:  2449.1523 Validation Accuracy: 0.667969\n",
      "Epoch  2, Batch  94 -Loss:  2929.9048 Validation Accuracy: 0.679688\n",
      "Epoch  2, Batch  95 -Loss:  2841.8398 Validation Accuracy: 0.683594\n",
      "Epoch  2, Batch  96 -Loss:  2749.7915 Validation Accuracy: 0.687500\n",
      "Epoch  2, Batch  97 -Loss:  2585.8999 Validation Accuracy: 0.687500\n",
      "Epoch  2, Batch  98 -Loss:  2777.5640 Validation Accuracy: 0.691406\n",
      "Epoch  2, Batch  99 -Loss:  2498.0610 Validation Accuracy: 0.691406\n",
      "Epoch  2, Batch 100 -Loss:  2351.1814 Validation Accuracy: 0.691406\n",
      "Epoch  2, Batch 101 -Loss:  2813.3066 Validation Accuracy: 0.691406\n",
      "Epoch  2, Batch 102 -Loss:  3105.5598 Validation Accuracy: 0.691406\n",
      "Epoch  2, Batch 103 -Loss:  2743.6260 Validation Accuracy: 0.687500\n",
      "Epoch  2, Batch 104 -Loss:  2816.2266 Validation Accuracy: 0.687500\n",
      "Epoch  2, Batch 105 -Loss:  2649.5684 Validation Accuracy: 0.695312\n",
      "Epoch  2, Batch 106 -Loss:  2136.5737 Validation Accuracy: 0.691406\n",
      "Epoch  2, Batch 107 -Loss:  2644.6270 Validation Accuracy: 0.687500\n",
      "Testing Accuracy: 0.6328125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 2\n",
    "batch_size = 512\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units\n",
    "#Weights and Biases\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')\n",
    "\n",
    "# conv_net \n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# Session!\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:test_valid_size],\n",
    "                y: mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[:test_valid_size],\n",
    "        y: mnist.test.labels[:test_valid_size],\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Convolution Layers in TensorFlow\n",
    "Let's now apply what we've learned to build real CNNs in TensorFlow. In the below exercise, you'll be asked to set up the dimensions of the Convolution filters, the weights, the biases. This is in many ways the trickiest part to using CNNs in TensorFlow. Once you have a sense of how to set up the dimensions of these attributes, applying CNNs will be far more straight forward.\n",
    "\n",
    "### Review  \n",
    "You should go over the TensorFlow documentation for 2D convolutions. Most of the documentation is straightforward, except perhaps the padding argument. The padding might differ depending on whether you pass 'VALID' or 'SAME'.\n",
    "\n",
    "Here are a few more things worth reviewing:\n",
    "    #1. Introduction to TensorFlow\n",
    "    #2. TensorFlow Variables.\n",
    "How to determine the dimensions of the output based on the input size and the filter size (shown below). You'll use this to determine what the size of your filter should be.  \n",
    "    #1. new_height = (input_height - filter_height + 2 P)/S + 1  \n",
    "    #2. new_width = (input_width - filter_width + 2 P)/S + 1  \n",
    "  \n",
    "Instructions\n",
    "Finish off each TODO in the conv2d function.\n",
    "Setup the strides, padding and filter weight/bias (F_w and F_b) such that the output shape is (1, 2, 2, 3). Note that all of these except strides should be TensorFlow variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ -3.37403059  -4.50971985   1.07491863]\n",
      "   [ -4.0476799   -5.34105396  -9.10417461]]\n",
      "\n",
      "  [[ -2.51265693  -9.7593441  -19.93506813]\n",
      "   [ -8.83568382 -15.85139465  -2.21849608]]]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup the strides, padding and filter weight/bias such that\n",
    "the output shape is (1, 2, 2, 3).\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array([\n",
    "    [0, 1, 0.5, 10],\n",
    "    [2, 2.5, 1, -8],\n",
    "    [4, 0, 5, 6],\n",
    "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "\n",
    "def conv2d(input):\n",
    "    # Filter (weights and bias)\n",
    "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "    # The shape of the filter bias is (output_depth,)\n",
    "    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\n",
    "    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n",
    "    # filter size must be [filter_height, filter_width, in_channels, out_channels]\n",
    "\n",
    "    F_W = tf.Variable(tf.truncated_normal([2,2,1,3]))\n",
    "    F_b = tf.Variable(tf.truncated_normal([3]))\n",
    "    # or F_b = tf.Variable(tf.zeros(3))\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [1, 2, 2, 1]\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = 'VALID'\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\n",
    "    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\n",
    "    return tf.nn.conv2d(input, F_W, strides, padding) + F_b\n",
    "\n",
    "out = conv2d(X)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    out2 = sess.run(out)\n",
    "    print(out2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pooling Layers in TensorFlow\n",
    "In the below exercise, you'll be asked to set up the dimensions of the pooling filters, strides, as well as the appropriate padding. You should go over the TensorFlow documentation for tf.nn.max_pool(). Padding works the same as it does for a convolution.\n",
    "\n",
    "### Instructions\n",
    "Finish off each TODO in the maxpool function.\n",
    "Setup the strides, padding and ksize such that the output shape after pooling is (1, 2, 2, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  2.5]\n",
      "   [ 10. ]]\n",
      "\n",
      "  [[ 15. ]\n",
      "   [  6. ]]]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the values to `strides` and `ksize` such that\n",
    "the output shape after pooling is (1, 2, 2, 1).\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array([\n",
    "    [0, 1, 0.5, 10],\n",
    "    [2, 2.5, 1, -8],\n",
    "    [4, 0, 5, 6],\n",
    "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "def maxpool(input):\n",
    "    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\n",
    "    ksize = [1, 2, 2, 1]\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [1, 2, 2, 1]\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = 'SAME'\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool\n",
    "    return tf.nn.max_pool(input, ksize, strides, padding)\n",
    "    \n",
    "out = maxpool(X)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "with open('small_train_traffic.p', mode='rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train, y_train = data['features'], data['labels']\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "# TODO: Build the Final Test Neural Network in Keras Here\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# preprocess data\n",
    "X_normalized = np.array(X_train / 255.0 - 0.5 )\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_one_hot = label_binarizer.fit_transform(y_train)\n",
    "\n",
    "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
    "history = model.fit(X_normalized, y_one_hot, nb_epoch=10, validation_split=0.2)\n",
    "\n",
    "with open('small_test_traffic.p', 'rb') as f:\n",
    "    data_test = pickle.load(f)\n",
    "\n",
    "X_test = data_test['features']\n",
    "y_test = data_test['labels']\n",
    "\n",
    "# preprocess data\n",
    "X_normalized_test = np.array(X_test / 255.0 - 0.5 )\n",
    "y_one_hot_test = label_binarizer.fit_transform(y_test)\n",
    "\n",
    "print(\"Testing\")\n",
    "\n",
    "# TODO: Evaluate the test data in Keras Here\n",
    "metrics = model.evaluate(X_normalized_test, y_one_hot_test)\n",
    "# TODO: UNCOMMENT CODE\n",
    "for metric_i in range(len(model.metrics_names)):\n",
    "    metric_name = model.metrics_names[metric_i]\n",
    "    metric_value = metrics[metric_i]\n",
    "    print('{}: {}'.format(metric_name, metric_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
